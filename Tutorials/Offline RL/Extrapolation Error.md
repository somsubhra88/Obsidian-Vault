The key insight in TD Learning is that $Q$-values for the current time step can be defined in terms of $Q$-values of the next time step. That is, $Q^{\pi}(s, a)$ is defined recursively, as below
$$
Q^\pi (s, a) = \mathbb{E}_{s^\prime \sim p(s^\prime \vert s, a), r \sim \mathcal{R}(s, a, s^\prime)} \Big[ r + \gamma \mathbb{E}_{a^\prime \sim \pi(s^\prime)} \big[Q^\pi (s^\prime , a^\prime)  \big] \Big]
$$
Assume we have a neural network to represent the $Q$-function, $Q_\theta$. In TD learning, $Q^\pi_{target}(s_t, a_t)$ is derived by estimating the right-hand side of the above equation using $Q_\theta$. At each training iteration, $\hat{Q}^\pi (s_t, a_t)$ is updated to bring it closer to $Q^\pi_{target} (s_t, a_t)$.

However, there are two problems with the above equation - 
- The first problem is the outer expectation $\mathbb{E}_{s^\prime \sim p(s^\prime \vert s, a), r \sim \mathcal{R}(s, a, s^\prime)} [ \cdots ]$ due to next state and reward. If the environment is deterministic, then it is correct to only consider the actual next state when calculating the expectation over the next states. However. if the environment is stochastic, this breaks down. Taking action $a$ in states $s$ could transition the environment into a number of different next states, but only one next state was actually observed at each step. This problem can be overcome by considering only one example - the one that actually happened. This does mean that the *Q-value estimate may have high variance if the environment is stochastic, but it helps make the estimation tractable*. Using one example to estimate the distribution over next states $s^\prime$ and rewards $r$, we can take the outer expectation out and the Bellman Equation can be written as - $Q^\pi (s, a) = r + \gamma \mathbb{E}_{a^\prime \sim \pi(s^\prime)} \big[Q^\pi (s^\prime , a^\prime)  \big]$.
- The Second problem is the inner expectation $\mathbb{E}_{a^\prime \sim \pi(s^\prime)} [ \cdots]$ of the equation over the actions. We have access to the $Q$-value estimates for all of the actions in the next state $s^\prime$. Problem arises from not knowing the probability distribution over actions which is needed to calculate the expectation. There are two ways to solve this problem, each corresponds to a different algorithm - 
	- **SARSA**: solution is to use the action actually taken in the next state, $a^\prime$, i.e.  $Q^\pi (s, a) \approx r + \gamma Q^\pi (s^\prime, a^\prime)$
	- **DQN**: solution is to use the maximum $Q$-value, , i.e.  $Q^\pi (s, a) \approx r + \gamma \; max_{a_i^\prime} \; Q^\pi (s^\prime, a_i^\prime)$

In most of the offline setting we don't have the luxury to interact with the environment and whatever data we have we need to train the model using that only. *Extrapolation Error* generally arises when the data doesn't have enough coverage for the next state and action pair, i.e., if the $(s^\prime, a^\prime)$ pair is not observed in the original data, in that case we want to extrapolate using functional approximation. 