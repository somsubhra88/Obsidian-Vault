# Offline (Batch) Reinforcement Learning
**Offline Reinforcement Learning**, also known as **Batch Reinforcement Learning**, is a variant of reinforcement learning that requires the agent to learn from a fixed batch of data _without_ exploration. In other words, how does one _maximally exploit_ a static dataset?

Offline RL algorithms (so far) have been built on top of standard off-policy Deep Reinforcement Learning (Deep RL) algorithms, which tend to optimize some form of a Bellman equation or TD difference error. Offline RL is therefore about deriving the best policy possible given the data.

## Off-Policy Deep Reinforcement Learning Without Exploration
Most “off-policy algorithms” in deep RL will fail when solely shown off-policy data due to _extrapolation error_, where state-action pairs $(s, a)$ outside the data batch can have arbitrarily inaccurate values, _which adversely affects algorithms that rely on propagating those values._ In the online setting, exploration would be able to correct for such values because one can get ground-truth rewards, but the offline case lacks that luxury.

The proposed algorithm is [[Batch Constrained deep Q-learning (BCQ)]]. The idea is to run normal Q-learning, but in the maximization step (which is normally $max_{a^\prime} Q(s^\prime,a^\prime)$), instead of considering the max over all possible actions, we want to only consider actions $a^\prime$ such that $(s^\prime,a^\prime)$ actually appeared in the batch of data. Or, in more realistic cases, eliminate actions which are _unlikely_ to be selected by the behavior policy $\pi_b$ (the policy that generated the static data).